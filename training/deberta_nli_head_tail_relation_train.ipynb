{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iris/anaconda3/envs/cpkl/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import ClassLabel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "import evaluate\n",
    "import time\n",
    "\n",
    "\n",
    "# fix seeding for pytorch and huggingface\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset config\n",
    "linking_data_path = \"./dimiss_items/data/model_gpt-3.5-turbo-0125\"\n",
    "TRAIN_DATA_PATH = f'{linking_data_path}/processed/gpt_label_full_train_df.json'\n",
    "VALID_DATA_PATH = f'{linking_data_path}/processed/gpt_label_full_valid_df.json'\n",
    "\n",
    "DS_TYPE = \"relation\" # \"full\" or \"head\" or \"tail\"\n",
    "USE_TAG = True\n",
    "\n",
    "LABEL_TO_ID = {\"entailment\": 0, \"not_entailment\": 1}\n",
    "ID_TO_LABEL = {0: \"entailment\", 1: \"not_entailment\"}\n",
    "\n",
    "COMFACT_LABEL_TO_ID = {True: 0, False: 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def replace_phrases(sentence, person_a_tag):\n",
    "    sentence = sentence.lower()\n",
    "    # Pattern to find \"I am\" and replace with \"person a is\"\n",
    "    sentence = re.sub(r\"\\bi am\\b\", f\"{person_a_tag} is\", sentence, flags=re.IGNORECASE)\n",
    "    # Pattern to find \"I was\" and replace with \"person a was\"\n",
    "    sentence = re.sub(r\"\\bi was\\b\", f\"{person_a_tag} was\", sentence, flags=re.IGNORECASE)\n",
    "    sentence = re.sub(r\"\\bi\\b\", person_a_tag, sentence, flags=re.IGNORECASE)\n",
    "    sentence = re.sub(r\"\\bmy\\b\", f\"{person_a_tag}'s\", sentence, flags=re.IGNORECASE)\n",
    "    sentence = sentence.replace('person a', person_a_tag)\n",
    "    return sentence\n",
    "\n",
    "\n",
    "class DatasetLoader:\n",
    "    def __init__(self, train_data_path: str, valid_data_path: str, ds_type: str, sample_size: int = None):\n",
    "        self.train_data_path = train_data_path\n",
    "        self.valid_data_path = valid_data_path\n",
    "        self.sample_size = sample_size\n",
    "        self.pa_tag, self.pb_tag = 'Person A', 'Person B'\n",
    "        self.ds_type = ds_type\n",
    "\n",
    "    def get_conv_from_text(self, example):\n",
    "        center_utter = \"\"\n",
    "        post_utters, future_utters = [], []\n",
    "        for x in example['text']:\n",
    "            if x['type'] == 'ut':\n",
    "                center_utter = x['utter']\n",
    "            elif '-' in x['type']:\n",
    "                post_utters.append(x['utter'])\n",
    "            elif '+' in x['type']:\n",
    "                future_utters.append(x['utter'])\n",
    "\n",
    "        convs_list = '\\n'.join(post_utters + [center_utter] + future_utters)\n",
    "        return convs_list\n",
    "\n",
    "    def get_conv_with_tag_from_text(self, example):\n",
    "        def format_utter(utter, tag):\n",
    "            if utter.strip() == '':\n",
    "                return ''\n",
    "            return f\"{tag}: {utter}\"\n",
    "        center_utter = \"\"\n",
    "        post_utters, future_utters = [], []\n",
    "        for x in example['text']:\n",
    "            if x['type'] == 'ut':\n",
    "                center_utter = format_utter(x['utter'], self.pa_tag)\n",
    "            elif '-1' in x['type']:\n",
    "                post_utters.append(format_utter(x['utter'], self.pb_tag))\n",
    "            elif '-2' in x['type']:\n",
    "                post_utters.append(format_utter(x['utter'], self.pa_tag)) \n",
    "            elif '+1' in x['type']:\n",
    "                future_utters.append(format_utter(x['utter'], self.pb_tag)) \n",
    "            elif '+2' in x['type']:\n",
    "                future_utters.append(format_utter(x['utter'], self.pa_tag)) \n",
    "\n",
    "        convs_list = '\\n'.join(post_utters + [center_utter] + future_utters)\n",
    "        return convs_list #.lower()\n",
    "\n",
    "    def transform_df(self, df):\n",
    "        df['conv'] = df.apply(self.get_conv_from_text, axis=1) if not USE_TAG else df.apply(self.get_conv_with_tag_from_text, axis=1)\n",
    "        df['fact_text'] = df['fact_text'].apply(lambda x: x) if not USE_TAG else df['fact_text'].apply(lambda x: replace_phrases(x, self.pa_tag))\n",
    "        df['labels'] = df['gold_reference'].apply(lambda x: COMFACT_LABEL_TO_ID[x])\n",
    "        df['label_text'] = df['labels'].apply(lambda x: ID_TO_LABEL[x].lower())\n",
    "        return df\n",
    "\n",
    "    def create_pd_dataframe(self, data_path, sample_size=None):\n",
    "        df = pd.read_json(data_path)\n",
    "        df = self.modify_merged_head_tail(df)\n",
    "        df = df.sample(sample_size) if sample_size else df\n",
    "        df = self.transform_df(df)\n",
    "        return df\n",
    "\n",
    "    def create_dataset(self, train_df, valid_df):\n",
    "        train_ds = Dataset.from_pandas(train_df)\n",
    "        valid_ds = Dataset.from_pandas(valid_df)\n",
    "\n",
    "        dataset = DatasetDict({\n",
    "            'train': train_ds,\n",
    "            'valid': valid_ds\n",
    "        })\n",
    "        return dataset\n",
    "\n",
    "    def modify_merged_head_tail(self, df):\n",
    "        if self.ds_type == \"head\":\n",
    "            df['gold_reference'] = df['gpt_tagged_head_gold_reference']\n",
    "            df['fact_text'] = df['gpt_tagged_head_fact_text']\n",
    "        elif self.ds_type == \"tail\":\n",
    "            df['gold_reference'] = df['gpt_tagged_tail_gold_reference']\n",
    "            df['fact_text'] = df['gpt_tagged_tail_fact_text']\n",
    "        elif self.ds_type == \"relation\":\n",
    "            df['gold_reference'] = df['gpt_tagged_head_gold_reference'] & df['gpt_tagged_tail_gold_reference']\n",
    "            df['fact_text'] = df.apply(lambda x: f\"{x['relation']} {x['gpt_tagged_head_fact_text']} and {x['gpt_tagged_tail_fact_text']}\", axis=1)\n",
    "        else:\n",
    "            df['gold_reference'] = df['gpt_tagged_head_gold_reference'] & df['gpt_tagged_tail_gold_reference']\n",
    "            df['fact_text'] = df.apply(lambda x: f\"{x['gpt_tagged_head_fact_text']} and {x['gpt_tagged_tail_fact_text']}\", axis=1)\n",
    "        return df\n",
    "\n",
    "    def load(self):\n",
    "        train_df = self.create_pd_dataframe(self.train_data_path)\n",
    "        valid_df = self.create_pd_dataframe(self.valid_data_path)\n",
    "        dataset = self.create_dataset(train_df, valid_df)\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['dialog_id', 'relation', 'head', 'tail', 'text', 'gpt_tagged_head_old_label', 'gpt_tagged_head_gpt_output', 'gpt_tagged_head_fact_text', 'gpt_tagged_head_gold_reference', 'gpt_tagged_tail_gpt_output', 'gpt_tagged_tail_old_label', 'gpt_tagged_tail_action', 'gpt_tagged_tail_fact_text', 'gpt_tagged_tail_gold_reference', 'gold_reference', 'fact_text', 'conv', 'labels', 'label_text', '__index_level_0__'],\n",
       "        num_rows: 35821\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['dialog_id', 'relation', 'head', 'tail', 'text', 'gpt_tagged_head_old_label', 'gpt_tagged_head_gpt_output', 'gpt_tagged_head_fact_text', 'gpt_tagged_head_gold_reference', 'gpt_tagged_tail_gpt_output', 'gpt_tagged_tail_old_label', 'gpt_tagged_tail_action', 'gpt_tagged_tail_fact_text', 'gpt_tagged_tail_gold_reference', 'gold_reference', 'fact_text', 'conv', 'labels', 'label_text', '__index_level_0__'],\n",
       "        num_rows: 3981\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "dataset_loader = DatasetLoader(TRAIN_DATA_PATH, VALID_DATA_PATH, DS_TYPE)\n",
    "dataset = dataset_loader.load()\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'additional_special_tokens': ['routine_habit_relationship',\n",
       "  'experience_relationship',\n",
       "  'characteristic',\n",
       "  'goal_plan',\n",
       "  'goal_plan_relationship',\n",
       "  'routine_habit',\n",
       "  'characteristic_relationship',\n",
       "  'experience']}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use train_ds and valid_ds to get all unique relations for specical tokenization\n",
    "relations_tokens = set(dataset['train']['relation'] + dataset['valid']['relation'])\n",
    "relations_special_tokens = {'additional_special_tokens': list(relations_tokens)}\n",
    "relations_special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation ds train len is 35821\n",
      "relation Positive: 7922(0.22115518829736747)\n",
      "relation Negative: 27899(0.7788448117026325)\n",
      "\n",
      "relation ds valid len is 3981\n",
      "relation Positive: 863(0.2167797035920623)\n",
      "relation Negative: 3118(0.7832202964079377)\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "counter = Counter([example['gold_reference'] for example in dataset['train']])\n",
    "valid_counter = Counter([example['gold_reference'] for example in dataset['valid']])\n",
    "\n",
    "# Stats for positive and negiative pairs in dataset\n",
    "print(f\"{DS_TYPE} ds train len is {len(dataset['train'])}\")\n",
    "print(f\"{DS_TYPE} Positive: {counter[True]}({counter[True]/len(dataset['train'])})\")\n",
    "print(f\"{DS_TYPE} Negative: {counter[False]}({counter[False]/len(dataset['train'])})\")\n",
    "print()\n",
    "print(f\"{DS_TYPE} ds valid len is {len(dataset['valid'])}\")\n",
    "print(f\"{DS_TYPE} Positive: {valid_counter[True]}({valid_counter[True]/len(dataset['valid'])})\")\n",
    "print(f\"{DS_TYPE} Negative: {valid_counter[False]}({valid_counter[False]/len(dataset['valid'])})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Person A: i don't have hair. i want to go to vegas. how are you? just to let you know i'm single.\n",
      "Person B: are you male or female? i can not wait to go back to school. do you use a wig? winter and fall is my favorite season\n",
      "Person A: i saw a wig on netflix. fall isn't that good. male, college? you do sleeping in? i do\n",
      "Person B: my sister loves me, and i love her. we are best friends. why not? does it get you down?\n",
      "Person A: it is just that in vegas fall causes weird people to come out. you saw right? i'm happy for you. love is so hard to come by. \n",
      "\n",
      "am always there for friends\n",
      "Person A is a loyal person\n",
      "None\n",
      "routine_habit_relationship\n",
      "routine_habit_relationship Person A is a loyal Person And am always there for friends\n"
     ]
    }
   ],
   "source": [
    "# Select one example to see how it looks\n",
    "print(dataset['train'][0]['conv'], '\\n')\n",
    "print(dataset['train'][0]['gpt_tagged_tail_fact_text'])\n",
    "print(dataset['train'][0]['gpt_tagged_head_fact_text'])\n",
    "print(dataset['train'][0]['gpt_tagged_tail_action'])\n",
    "print(dataset['train'][0]['relation'])\n",
    "print(dataset['train'][0]['fact_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- Model Name: MoritzLaurer/deberta-v3-large-zeroshot-v1.1-all-33\n",
      "- Model Size: large\n",
      "- Ds Type: relation\n",
      "- Output Dir: ./dimiss_items/model/nli_relation/deberta-v3-large-1720917183/\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mirislin1006\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/iris/Desktop/wsl_shared/CPKL/wandb/run-20240713_193304-djzxtmov</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/irislin1006/cpkl-relation-MoritzLaurer-deberta-v3-large-zeroshot-v1.1-all-33/runs/djzxtmov' target=\"_blank\">deberta-v3-large-relation</a></strong> to <a href='https://wandb.ai/irislin1006/cpkl-relation-MoritzLaurer-deberta-v3-large-zeroshot-v1.1-all-33' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/irislin1006/cpkl-relation-MoritzLaurer-deberta-v3-large-zeroshot-v1.1-all-33' target=\"_blank\">https://wandb.ai/irislin1006/cpkl-relation-MoritzLaurer-deberta-v3-large-zeroshot-v1.1-all-33</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/irislin1006/cpkl-relation-MoritzLaurer-deberta-v3-large-zeroshot-v1.1-all-33/runs/djzxtmov' target=\"_blank\">https://wandb.ai/irislin1006/cpkl-relation-MoritzLaurer-deberta-v3-large-zeroshot-v1.1-all-33/runs/djzxtmov</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/irislin1006/cpkl-relation-MoritzLaurer-deberta-v3-large-zeroshot-v1.1-all-33/runs/djzxtmov?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f65e1d15940>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model config\n",
    "IS_TRAIN_SAMPLED = False\n",
    "MODEL_SIZE = \"large\"\n",
    "# MODEL_NAME = f\"microsoft/deberta-v3-{MODEL_SIZE}\"\n",
    "MODEL_NAME = f\"MoritzLaurer/deberta-v3-large-zeroshot-v1.1-all-33\"\n",
    "OUTPUT_DIR = f\"./dimiss_items/model/nli_{DS_TYPE}/deberta-v3-{MODEL_SIZE}-{str(int(time.time()))}/\"\n",
    "\n",
    "print(f\"\"\"\n",
    "- Model Name: {MODEL_NAME}\n",
    "- Model Size: {MODEL_SIZE}\n",
    "- Ds Type: {DS_TYPE}\n",
    "- Output Dir: {OUTPUT_DIR}\n",
    "\"\"\")\n",
    "\n",
    "import wandb\n",
    "wandb.init(project=f\"cpkl-{DS_TYPE}-{MODEL_NAME.replace('/', '-')}\", name=f\"deberta-v3-{MODEL_SIZE}-{DS_TYPE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_name, label2id, id2label):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False, model_max_length=512)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, label2id=label2id, id2label=id2label)\n",
    "    \n",
    "    if DS_TYPE == \"relation\":\n",
    "        tokenizer.add_special_tokens(relations_special_tokens)\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False, model_max_length=512)\n",
    "# \n",
    "model, tokenizer = load_model_and_tokenizer(MODEL_NAME, LABEL_TO_ID, ID_TO_LABEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Dataset for trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 35821/35821 [00:13<00:00, 2602.41 examples/s]\n",
      "Map: 100%|██████████| 3981/3981 [00:01<00:00, 2612.06 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using relation Dataset. Keys of tokenized dataset: ['labels', 'label_text', 'input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "   return tokenizer(examples[\"conv\"], examples[\"fact_text\"], padding=True, truncation=True, max_length=512)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\n",
    "   'dialog_id', \n",
    "   'text',\n",
    "   'relation',\n",
    "   'head',\n",
    "   'tail',\n",
    "   'gpt_tagged_head_old_label', \n",
    "   'gpt_tagged_head_gold_reference', \n",
    "   'gpt_tagged_head_fact_text', \n",
    "   'gpt_tagged_head_gpt_output', \n",
    "   'gpt_tagged_tail_old_label', \n",
    "   'gpt_tagged_tail_gold_reference', \n",
    "   'gpt_tagged_tail_fact_text', \n",
    "   'gpt_tagged_tail_gpt_output', \n",
    "   'gpt_tagged_tail_action', \n",
    "   'fact_text', \n",
    "   'gold_reference', \n",
    "   'conv',\n",
    "   '__index_level_0__'])\n",
    "print(f\"Using {DS_TYPE} Dataset. Keys of tokenized dataset: {list(tokenized_dataset['train'].features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare trainer\n",
    "\n",
    "acc = evaluate.load(\"accuracy\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "roc_auc_score = evaluate.load(\"roc_auc\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    labels = eval_preds.label_ids\n",
    "    pred_logits = eval_preds.predictions\n",
    "    preds_max = np.argmax(pred_logits, axis=1)  # argmax on each row (axis=1) in the tensor\n",
    "    pred_probs = torch.softmax(torch.tensor(pred_logits), dim=1)[:, 0].numpy()\n",
    "      \n",
    "    print(\"Number of predictions: \", len(preds_max))\n",
    "    # compute f1, precision, recall, and accuracy \n",
    "    result = {}\n",
    "    result.update(acc.compute(predictions=preds_max, references=labels))\n",
    "    result.update(f1.compute(predictions=preds_max, references=labels, average='binary', pos_label=0))\n",
    "    result.update(precision.compute(predictions=preds_max, references=labels, average='binary', pos_label=0))\n",
    "    result.update(recall.compute(predictions=preds_max, references=labels, average='binary', pos_label=0))\n",
    "    result.update(roc_auc_score.compute(prediction_scores=pred_probs, references=1-labels))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trainer_args(output_dir):\n",
    "    if DS_TYPE == \"full\":\n",
    "        eval_steps=1500\n",
    "        save_steps=1500\n",
    "    elif DS_TYPE == \"head\":\n",
    "        eval_steps=1500\n",
    "        save_steps=1500\n",
    "    elif DS_TYPE == \"tail\":\n",
    "        # eval_steps=500\n",
    "        # save_steps=500\n",
    "        eval_steps=1500\n",
    "        save_steps=1500\n",
    "    elif DS_TYPE == \"relation\":\n",
    "        eval_steps=3000\n",
    "        save_steps=3000\n",
    "    else:\n",
    "        raise ValueError(f\"'{DS_TYPE}' is invalid dataset type. Must be one of 'full', 'head', 'tail'\")\n",
    "    \n",
    "    return TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        bf16 =False, # Overflows with bf16 \n",
    "        learning_rate=1e-6,\n",
    "        warmup_ratio=0.01,\n",
    "        weight_decay=0.01,\n",
    "        num_train_epochs=3,\n",
    "        # logging & evaluation strategies\n",
    "        logging_dir=f\"{output_dir}/logs\",\n",
    "        logging_strategy=\"steps\", \n",
    "        logging_steps=20,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=eval_steps,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=save_steps,\n",
    "        save_total_limit=5,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"roc_auc\",\n",
    "        # push to hub parameters\n",
    "        report_to=\"wandb\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trainer(model, tokenized_dataset, output_dir):\n",
    "    training_args = get_trainer_args(output_dir)\n",
    "    trainer =Trainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset[\"train\"],\n",
    "        eval_dataset=tokenized_dataset[\"valid\"],\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    return trainer\n",
    "\n",
    "trainer = get_trainer(model, tokenized_dataset, OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13434' max='13434' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13434/13434 1:53:47, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Roc Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.329700</td>\n",
       "      <td>0.337791</td>\n",
       "      <td>0.853052</td>\n",
       "      <td>0.652406</td>\n",
       "      <td>0.669512</td>\n",
       "      <td>0.636153</td>\n",
       "      <td>0.896907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.357700</td>\n",
       "      <td>0.351733</td>\n",
       "      <td>0.856820</td>\n",
       "      <td>0.669757</td>\n",
       "      <td>0.669757</td>\n",
       "      <td>0.669757</td>\n",
       "      <td>0.902930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.265100</td>\n",
       "      <td>0.351695</td>\n",
       "      <td>0.860839</td>\n",
       "      <td>0.664649</td>\n",
       "      <td>0.695817</td>\n",
       "      <td>0.636153</td>\n",
       "      <td>0.904400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.294200</td>\n",
       "      <td>0.378534</td>\n",
       "      <td>0.856820</td>\n",
       "      <td>0.672037</td>\n",
       "      <td>0.667429</td>\n",
       "      <td>0.676709</td>\n",
       "      <td>0.904264</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of predictions:  3981\n",
      "Number of predictions:  3981\n",
      "Number of predictions:  3981\n",
      "Number of predictions:  3981\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=13434, training_loss=0.35441395670987497, metrics={'train_runtime': 6828.695, 'train_samples_per_second': 15.737, 'train_steps_per_second': 1.967, 'total_flos': 5.21995717871731e+16, 'train_loss': 0.35441395670987497, 'epoch': 3.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of predictions:  3981\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot a roc curve with sk learn  and matplot lib\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_roc_curve(trainer, tokenized_dataset):\n",
    "    output = trainer.predict(tokenized_dataset[\"valid\"])\n",
    "    labels = 1 - output.label_ids\n",
    "    pred_logits = output.predictions\n",
    "    pred_probs = torch.softmax(torch.tensor(pred_logits), dim=1)[:, 0].numpy()\n",
    "    fpr, tpr, thresholds = roc_curve(labels, pred_probs, pos_label=1)\n",
    "    plt.plot(fpr, tpr, label=f\"ROC curve (area = {auc(fpr, tpr):.2f})\")\n",
    "    return pred_probs, thresholds\n",
    "\n",
    "pred_probs, thres = plot_roc_curve(trainer, tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 0.2\n",
      "{'accuracy': 0.8467721677970359, 'f1': 0.6765641569459172, 'precision': 0.6236559139784946, 'recall': 0.7392815758980301, 'roc_auc': 0.9043995281760228}\n",
      "Threshold: 0.3\n",
      "{'accuracy': 0.8558151218286862, 'f1': 0.6746031746031746, 'precision': 0.660377358490566, 'recall': 0.6894553881807648, 'roc_auc': 0.9043995281760228}\n",
      "Threshold: 0.4\n",
      "{'accuracy': 0.8580758603365989, 'f1': 0.6682325308279508, 'precision': 0.6773809523809524, 'recall': 0.6593279258400927, 'roc_auc': 0.9043995281760228}\n",
      "Threshold: 0.5\n",
      "{'accuracy': 0.8608389851796031, 'f1': 0.6646489104116223, 'precision': 0.6958174904942965, 'recall': 0.6361529548088065, 'roc_auc': 0.9043995281760228}\n",
      "Threshold: 0.6\n",
      "{'accuracy': 0.8623461441848782, 'f1': 0.6562107904642409, 'precision': 0.7154582763337893, 'recall': 0.6060254924681344, 'roc_auc': 0.9043995281760228}\n",
      "Threshold: 0.7\n",
      "{'accuracy': 0.8620949510173324, 'f1': 0.6366644606221046, 'precision': 0.7422839506172839, 'recall': 0.5573580533024334, 'roc_auc': 0.9043995281760228}\n"
     ]
    }
   ],
   "source": [
    "# compute f1, precision, recall, and accuracy by given threshold\n",
    "def compute_metrics_with_threshold(pred_probs, thres, labels):\n",
    "    preds = (pred_probs > thres).astype(int)\n",
    "    result = {}\n",
    "    result.update(acc.compute(predictions=preds, references=labels))\n",
    "    result.update(f1.compute(predictions=preds, references=labels, average='binary', pos_label=1))\n",
    "    result.update(precision.compute(predictions=preds, references=labels, average='binary', pos_label=1))\n",
    "    result.update(recall.compute(predictions=preds, references=labels, average='binary', pos_label=1))\n",
    "    result.update(roc_auc_score.compute(prediction_scores=pred_probs, references=labels))\n",
    "    return result\n",
    "\n",
    "for t in [0.2, 0.3, 0.4, 0.5, 0.6, 0.7]:\n",
    "    print(f\"Threshold: {t}\")\n",
    "    print(compute_metrics_with_threshold(pred_probs, t, 1 - np.array(tokenized_dataset[\"valid\"][\"labels\"]).astype(int)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./dimiss_items/model/nli_relation/deberta-v3-large-1720917183/'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error (ReadTimeout), entering retry loop.\n"
     ]
    }
   ],
   "source": [
    "def save_model(model, tokenizer, output_dir):\n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "# OUTPUT_DIR = 'agents/dimiss_items/model/full/deberta-v3-large-1710086328/'\n",
    "save_model_path = OUTPUT_DIR + f\"deberta_v3_large_sample_{IS_TRAIN_SAMPLED}\"\n",
    "save_model(model, tokenizer, save_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpkl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
