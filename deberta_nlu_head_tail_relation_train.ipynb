{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iris/anaconda3/envs/cpkl/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import ClassLabel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "import evaluate\n",
    "import time\n",
    "\n",
    "\n",
    "# fix seeding for pytorch and huggingface\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset config\n",
    "linking_data_path = \"./dimiss_items/data/model_gpt-3.5-turbo-0125\"\n",
    "TRAIN_DATA_PATH = f'{linking_data_path}/processed/gpt_label_full_train_df.json'\n",
    "VALID_DATA_PATH = f'{linking_data_path}/processed/gpt_label_full_valid_df.json'\n",
    "\n",
    "DS_TYPE = \"relation\" # \"full\" or \"head\" or \"tail\"\n",
    "USE_TAG = True\n",
    "\n",
    "LABEL_TO_ID = {\"entailment\": 0, \"not_entailment\": 1}\n",
    "ID_TO_LABEL = {0: \"entailment\", 1: \"not_entailment\"}\n",
    "\n",
    "COMFACT_LABEL_TO_ID = {True: 0, False: 1} # comFactDataLabelToId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def replace_phrases(sentence, person_a_tag):\n",
    "    sentence = sentence.lower()\n",
    "    # Pattern to find \"I am\" and replace with \"person a is\"\n",
    "    sentence = re.sub(r\"\\bi am\\b\", f\"{person_a_tag} is\", sentence, flags=re.IGNORECASE)\n",
    "    # Pattern to find \"I was\" and replace with \"person a was\"\n",
    "    sentence = re.sub(r\"\\bi was\\b\", f\"{person_a_tag} was\", sentence, flags=re.IGNORECASE)\n",
    "    sentence = re.sub(r\"\\bi\\b\", person_a_tag, sentence, flags=re.IGNORECASE)\n",
    "    sentence = re.sub(r\"\\bmy\\b\", f\"{person_a_tag}'s\", sentence, flags=re.IGNORECASE)\n",
    "    sentence = sentence.replace('person a', person_a_tag)\n",
    "    return sentence\n",
    "\n",
    "\n",
    "class DatasetLoader:\n",
    "    def __init__(self, train_data_path: str, valid_data_path: str, ds_type: str, sample_size: int = None):\n",
    "        self.train_data_path = train_data_path\n",
    "        self.valid_data_path = valid_data_path\n",
    "        self.sample_size = sample_size\n",
    "        self.pa_tag, self.pb_tag = 'Person A', 'Person B'\n",
    "        self.ds_type = ds_type\n",
    "\n",
    "    def get_conv_from_text(self, example):\n",
    "        center_utter = \"\"\n",
    "        post_utters, future_utters = [], []\n",
    "        for x in example['text']:\n",
    "            if x['type'] == 'ut':\n",
    "                center_utter = x['utter']\n",
    "            elif '-' in x['type']:\n",
    "                post_utters.append(x['utter'])\n",
    "            elif '+' in x['type']:\n",
    "                future_utters.append(x['utter'])\n",
    "\n",
    "        convs_list = '\\n'.join(post_utters + [center_utter] + future_utters)\n",
    "        return convs_list\n",
    "\n",
    "    def get_conv_with_tag_from_text(self, example):\n",
    "        def format_utter(utter, tag):\n",
    "            if utter.strip() == '':\n",
    "                return ''\n",
    "            return f\"{tag}: {utter}\"\n",
    "        center_utter = \"\"\n",
    "        post_utters, future_utters = [], []\n",
    "        for x in example['text']:\n",
    "            if x['type'] == 'ut':\n",
    "                center_utter = format_utter(x['utter'], self.pa_tag)\n",
    "            elif '-1' in x['type']:\n",
    "                post_utters.append(format_utter(x['utter'], self.pb_tag))\n",
    "            elif '-2' in x['type']:\n",
    "                post_utters.append(format_utter(x['utter'], self.pa_tag)) \n",
    "            elif '+1' in x['type']:\n",
    "                future_utters.append(format_utter(x['utter'], self.pb_tag)) \n",
    "            elif '+2' in x['type']:\n",
    "                future_utters.append(format_utter(x['utter'], self.pa_tag)) \n",
    "\n",
    "        convs_list = '\\n'.join(post_utters + [center_utter] + future_utters)\n",
    "        return convs_list #.lower()\n",
    "\n",
    "    def transform_df(self, df):\n",
    "        df['conv'] = df.apply(self.get_conv_from_text, axis=1) if not USE_TAG else df.apply(self.get_conv_with_tag_from_text, axis=1)\n",
    "        df['fact_text'] = df['fact_text'].apply(lambda x: x) if not USE_TAG else df['fact_text'].apply(lambda x: replace_phrases(x, self.pa_tag))\n",
    "        df['labels'] = df['gold_reference'].apply(lambda x: COMFACT_LABEL_TO_ID[x])\n",
    "        df['label_text'] = df['labels'].apply(lambda x: ID_TO_LABEL[x].lower())\n",
    "        return df\n",
    "\n",
    "    def create_pd_dataframe(self, data_path, sample_size=None):\n",
    "        df = pd.read_json(data_path)\n",
    "        # if self.ds_type == \"relation\":\n",
    "        #     df['peacok_relation'] = df['relation']\n",
    "        # if 'merged_head_tail' in data_path:\n",
    "        #     df = self.modify_merged_head_tail(df)\n",
    "        df = self.modify_merged_head_tail(df)\n",
    "        df = df.sample(sample_size) if sample_size else df\n",
    "        df = self.transform_df(df)\n",
    "        return df\n",
    "\n",
    "    def create_dataset(self, train_df, valid_df):\n",
    "        train_ds = Dataset.from_pandas(train_df)\n",
    "        valid_ds = Dataset.from_pandas(valid_df)\n",
    "\n",
    "        dataset = DatasetDict({\n",
    "            'train': train_ds,\n",
    "            'valid': valid_ds\n",
    "        })\n",
    "        return dataset\n",
    "    \n",
    "    # def get_relation(self, df):\n",
    "    #     return origonal_peacok_relation_df[origonal_peacok_relation_df['dialog_id'].isin(df['dialog_id'])]['peacok_relation']\n",
    "    \n",
    "    def modify_merged_head_tail(self, df):\n",
    "        if self.ds_type == \"head\":\n",
    "            df['gold_reference'] = df['gpt_tagged_head_gold_reference']\n",
    "            df['fact_text'] = df['gpt_tagged_head_fact_text']\n",
    "        elif self.ds_type == \"tail\":\n",
    "            df['gold_reference'] = df['gpt_tagged_tail_gold_reference']\n",
    "            df['fact_text'] = df['gpt_tagged_tail_fact_text']\n",
    "        elif self.ds_type == \"relation\":\n",
    "            # assert df['head_text'].equals(df['tail_text']), \"head and tail text should be the same\"\n",
    "            # df['text'] = df['tail_text']\n",
    "            df['gold_reference'] = df['gpt_tagged_head_gold_reference'] & df['gpt_tagged_tail_gold_reference']\n",
    "            df['fact_text'] = df.apply(lambda x: f\"{x['relation']} {x['gpt_tagged_head_fact_text']} and {x['gpt_tagged_tail_fact_text']}\", axis=1)\n",
    "            # df['fact_text'] = df.apply(lambda x: f\"{x['head_fact_text']} and {x['tail_fact_text']}; {x['peacok_relation']}\", axis=1)\n",
    "        else:\n",
    "            # assert df['head_text'].equals(df['tail_text']), \"head and tail text should be the same\"\n",
    "            # df['text'] = df['tail_text']\n",
    "            df['gold_reference'] = df['gpt_tagged_head_gold_reference'] & df['gpt_tagged_tail_gold_reference']\n",
    "            df['fact_text'] = df.apply(lambda x: f\"{x['gpt_tagged_head_fact_text']} and {x['gpt_tagged_tail_fact_text']}\", axis=1)\n",
    "        return df\n",
    "\n",
    "    def load(self):\n",
    "        train_df = self.create_pd_dataframe(self.train_data_path) # create_pd_dataframe(TRAIN_DATA_PATH, 5000)\n",
    "        valid_df = self.create_pd_dataframe(self.valid_data_path) # create_pd_dataframe(VALID_DATA_PATH, 500)\n",
    "        dataset = self.create_dataset(train_df, valid_df)\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['dialog_id', 'relation', 'head', 'tail', 'text', 'gpt_tagged_head_old_label', 'gpt_tagged_head_gpt_output', 'gpt_tagged_head_fact_text', 'gpt_tagged_head_gold_reference', 'gpt_tagged_tail_gpt_output', 'gpt_tagged_tail_old_label', 'gpt_tagged_tail_action', 'gpt_tagged_tail_fact_text', 'gpt_tagged_tail_gold_reference', 'gold_reference', 'fact_text', 'conv', 'labels', 'label_text', '__index_level_0__'],\n",
       "        num_rows: 35821\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['dialog_id', 'relation', 'head', 'tail', 'text', 'gpt_tagged_head_old_label', 'gpt_tagged_head_gpt_output', 'gpt_tagged_head_fact_text', 'gpt_tagged_head_gold_reference', 'gpt_tagged_tail_gpt_output', 'gpt_tagged_tail_old_label', 'gpt_tagged_tail_action', 'gpt_tagged_tail_fact_text', 'gpt_tagged_tail_gold_reference', 'gold_reference', 'fact_text', 'conv', 'labels', 'label_text', '__index_level_0__'],\n",
       "        num_rows: 3981\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "dataset_loader = DatasetLoader(TRAIN_DATA_PATH, VALID_DATA_PATH, DS_TYPE)\n",
    "dataset = dataset_loader.load()\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'additional_special_tokens': ['experience',\n",
       "  'experience_relationship',\n",
       "  'goal_plan_relationship',\n",
       "  'routine_habit_relationship',\n",
       "  'routine_habit',\n",
       "  'goal_plan',\n",
       "  'characteristic_relationship',\n",
       "  'characteristic']}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use train_ds and valid_ds to get all unique relations for specical tokenization\n",
    "relations_tokens = set(dataset['train']['relation'] + dataset['valid']['relation'])\n",
    "relations_special_tokens = {'additional_special_tokens': list(relations_tokens)}\n",
    "relations_special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation ds train len is 35821\n",
      "relation Positive: 7922(0.22115518829736747)\n",
      "relation Negative: 27899(0.7788448117026325)\n",
      "\n",
      "relation ds valid len is 3981\n",
      "relation Positive: 863(0.2167797035920623)\n",
      "relation Negative: 3118(0.7832202964079377)\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "counter = Counter([example['gold_reference'] for example in dataset['train']])\n",
    "valid_counter = Counter([example['gold_reference'] for example in dataset['valid']])\n",
    "\n",
    "print(f\"{DS_TYPE} ds train len is {len(dataset['train'])}\")\n",
    "print(f\"{DS_TYPE} Positive: {counter[True]}({counter[True]/len(dataset['train'])})\")\n",
    "print(f\"{DS_TYPE} Negative: {counter[False]}({counter[False]/len(dataset['train'])})\")\n",
    "print()\n",
    "print(f\"{DS_TYPE} ds valid len is {len(dataset['valid'])}\")\n",
    "print(f\"{DS_TYPE} Positive: {valid_counter[True]}({valid_counter[True]/len(dataset['valid'])})\")\n",
    "print(f\"{DS_TYPE} Negative: {valid_counter[False]}({valid_counter[False]/len(dataset['valid'])})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Person A: i don't have hair. i want to go to vegas. how are you? just to let you know i'm single.\n",
      "Person B: are you male or female? i can not wait to go back to school. do you use a wig? winter and fall is my favorite season\n",
      "Person A: i saw a wig on netflix. fall isn't that good. male, college? you do sleeping in? i do\n",
      "Person B: my sister loves me, and i love her. we are best friends. why not? does it get you down?\n",
      "Person A: it is just that in vegas fall causes weird people to come out. you saw right? i'm happy for you. love is so hard to come by. \n",
      "\n",
      "am always there for friends\n",
      "Person A is a loyal person\n",
      "None\n",
      "routine_habit_relationship\n",
      "routine_habit_relationship Person A is a loyal Person And am always there for friends\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train'][0]['conv'], '\\n')\n",
    "print(dataset['train'][0]['gpt_tagged_tail_fact_text'])\n",
    "print(dataset['train'][0]['gpt_tagged_head_fact_text'])\n",
    "print(dataset['train'][0]['gpt_tagged_tail_action'])\n",
    "print(dataset['train'][0]['relation'])\n",
    "print(dataset['train'][0]['fact_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- Model Name: microsoft/deberta-v3-large\n",
      "- Model Size: large\n",
      "- Ds Type: relation\n",
      "- Output Dir: ./dimiss_items/model/relation/deberta-v3-large-1720838176/\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mirislin1006\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/iris/Desktop/wsl_shared/CPKL/wandb/run-20240712_213617-2v7iv4b6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/irislin1006/cpkl-relation-microsoft-deberta-v3-large/runs/2v7iv4b6' target=\"_blank\">deberta-v3-large-relation</a></strong> to <a href='https://wandb.ai/irislin1006/cpkl-relation-microsoft-deberta-v3-large' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/irislin1006/cpkl-relation-microsoft-deberta-v3-large' target=\"_blank\">https://wandb.ai/irislin1006/cpkl-relation-microsoft-deberta-v3-large</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/irislin1006/cpkl-relation-microsoft-deberta-v3-large/runs/2v7iv4b6' target=\"_blank\">https://wandb.ai/irislin1006/cpkl-relation-microsoft-deberta-v3-large/runs/2v7iv4b6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/irislin1006/cpkl-relation-microsoft-deberta-v3-large/runs/2v7iv4b6?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7efe22f3cfa0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model config\n",
    "IS_TRAIN_SAMPLED = False\n",
    "MODEL_SIZE = \"large\"\n",
    "MODEL_NAME = f\"microsoft/deberta-v3-{MODEL_SIZE}\"\n",
    "# MODEL_NAME = f\"MoritzLaurer/deberta-v3-large-zeroshot-v1.1-all-33\"\n",
    "OUTPUT_DIR = f\"./dimiss_items/model/{DS_TYPE}/deberta-v3-{MODEL_SIZE}-{str(int(time.time()))}/\"\n",
    "\n",
    "print(f\"\"\"\n",
    "- Model Name: {MODEL_NAME}\n",
    "- Model Size: {MODEL_SIZE}\n",
    "- Ds Type: {DS_TYPE}\n",
    "- Output Dir: {OUTPUT_DIR}\n",
    "\"\"\")\n",
    "\n",
    "import wandb\n",
    "wandb.init(project=f\"cpkl-{DS_TYPE}-{MODEL_NAME.replace('/', '-')}\", name=f\"deberta-v3-{MODEL_SIZE}-{DS_TYPE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_name, label2id, id2label):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False, model_max_length=512)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, label2id=label2id, id2label=id2label)\n",
    "    \n",
    "    if DS_TYPE == \"relation\":\n",
    "        tokenizer.add_special_tokens(relations_special_tokens)\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iris/anaconda3/envs/cpkl/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False, model_max_length=512)\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, label2id=label2id, id2label=id2label)\n",
    "model, tokenizer = load_model_and_tokenizer(MODEL_NAME, LABEL_TO_ID, ID_TO_LABEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Dataset for trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 35821/35821 [00:13<00:00, 2661.35 examples/s]\n",
      "Map: 100%|██████████| 3981/3981 [00:01<00:00, 2643.82 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using relation Dataset. Keys of tokenized dataset: ['labels', 'label_text', 'input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "   return tokenizer(examples[\"conv\"], examples[\"fact_text\"], padding=True, truncation=True, max_length=512)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\n",
    "   'dialog_id', \n",
    "   'text',\n",
    "   'relation',\n",
    "   'head',\n",
    "   'tail',\n",
    "   'gpt_tagged_head_old_label', \n",
    "   'gpt_tagged_head_gold_reference', \n",
    "   'gpt_tagged_head_fact_text', \n",
    "   'gpt_tagged_head_gpt_output', \n",
    "   'gpt_tagged_tail_old_label', \n",
    "   'gpt_tagged_tail_gold_reference', \n",
    "   'gpt_tagged_tail_fact_text', \n",
    "   'gpt_tagged_tail_gpt_output', \n",
    "   'gpt_tagged_tail_action', \n",
    "   'fact_text', \n",
    "   'gold_reference', \n",
    "   'conv',\n",
    "   '__index_level_0__'])\n",
    "print(f\"Using {DS_TYPE} Dataset. Keys of tokenized dataset: {list(tokenized_dataset['train'].features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare trainer\n",
    "\n",
    "acc = evaluate.load(\"accuracy\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "roc_auc_score = evaluate.load(\"roc_auc\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    labels = eval_preds.label_ids\n",
    "    pred_logits = eval_preds.predictions\n",
    "    preds_max = np.argmax(pred_logits, axis=1)  # argmax on each row (axis=1) in the tensor\n",
    "    pred_probs = torch.softmax(torch.tensor(pred_logits), dim=1)[:, 0].numpy()\n",
    "      \n",
    "    print(\"Number of predictions: \", len(preds_max))\n",
    "    # compute f1, precision, recall, and accuracy \n",
    "    result = {}\n",
    "    result.update(acc.compute(predictions=preds_max, references=labels))\n",
    "    result.update(f1.compute(predictions=preds_max, references=labels, average='binary', pos_label=0))\n",
    "    result.update(precision.compute(predictions=preds_max, references=labels, average='binary', pos_label=0))\n",
    "    result.update(recall.compute(predictions=preds_max, references=labels, average='binary', pos_label=0))\n",
    "    result.update(roc_auc_score.compute(prediction_scores=pred_probs, references=1-labels))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trainer_args(output_dir):\n",
    "    if DS_TYPE == \"full\":\n",
    "        eval_steps=1500\n",
    "        save_steps=1500\n",
    "    elif DS_TYPE == \"head\":\n",
    "        eval_steps=1500\n",
    "        save_steps=1500\n",
    "    elif DS_TYPE == \"tail\":\n",
    "        # eval_steps=500\n",
    "        # save_steps=500\n",
    "        eval_steps=1500\n",
    "        save_steps=1500\n",
    "    elif DS_TYPE == \"relation\":\n",
    "        # eval_steps=500\n",
    "        # save_steps=500\n",
    "        eval_steps=1500\n",
    "        save_steps=1500\n",
    "    else:\n",
    "        raise ValueError(f\"'{DS_TYPE}' is invalid dataset type. Must be one of 'full', 'head', 'tail'\")\n",
    "    \n",
    "    return TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        bf16 =False, # Overflows with bf16 \n",
    "        learning_rate=1e-6,\n",
    "        warmup_ratio=0.01,\n",
    "        weight_decay=0.01,\n",
    "        num_train_epochs=3,\n",
    "        # logging & evaluation strategies\n",
    "        logging_dir=f\"{output_dir}/logs\",\n",
    "        logging_strategy=\"steps\", \n",
    "        logging_steps=20,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=eval_steps,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=save_steps,\n",
    "        save_total_limit=5,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"roc_auc\",\n",
    "        # push to hub parameters\n",
    "        report_to=\"wandb\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trainer(model, tokenized_dataset, output_dir):\n",
    "    training_args = get_trainer_args(output_dir)\n",
    "    trainer =Trainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset[\"train\"],\n",
    "        eval_dataset=tokenized_dataset[\"valid\"],\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    return trainer\n",
    "\n",
    "trainer = get_trainer(model, tokenized_dataset, OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='56' max='13434' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   56/13434 00:26 < 1:49:40, 2.03 it/s, Epoch 0.01/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a roc curve with sk learn  and matplot lib\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_roc_curve(trainer, tokenized_dataset):\n",
    "    output = trainer.predict(tokenized_dataset[\"valid\"])\n",
    "    labels = 1 - output.label_ids\n",
    "    pred_logits = output.predictions\n",
    "    pred_probs = torch.softmax(torch.tensor(pred_logits), dim=1)[:, 0].numpy()\n",
    "    fpr, tpr, thresholds = roc_curve(labels, pred_probs, pos_label=1)\n",
    "    plt.plot(fpr, tpr, label=f\"ROC curve (area = {auc(fpr, tpr):.2f})\")\n",
    "    return pred_probs, thresholds\n",
    "\n",
    "pred_probs, thres = plot_roc_curve(trainer, tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute f1, precision, recall, and accuracy by given threshold\n",
    "def compute_metrics_with_threshold(pred_probs, thres, labels):\n",
    "    preds = (pred_probs > thres).astype(int)\n",
    "    result = {}\n",
    "    result.update(acc.compute(predictions=preds, references=labels))\n",
    "    result.update(f1.compute(predictions=preds, references=labels, average='binary', pos_label=1))\n",
    "    result.update(precision.compute(predictions=preds, references=labels, average='binary', pos_label=1))\n",
    "    result.update(recall.compute(predictions=preds, references=labels, average='binary', pos_label=1))\n",
    "    result.update(roc_auc_score.compute(prediction_scores=pred_probs, references=labels))\n",
    "    return result\n",
    "\n",
    "for t in [0.2, 0.3, 0.4, 0.5, 0.6, 0.7]:\n",
    "    print(f\"Threshold: {t}\")\n",
    "    print(compute_metrics_with_threshold(pred_probs, t, 1 - np.array(tokenized_dataset[\"valid\"][\"labels\"]).astype(int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpkl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
